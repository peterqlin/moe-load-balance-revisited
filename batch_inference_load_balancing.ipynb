{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "v1-VAi1wzOi6",
        "fkBsETQLuWUH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Qwen3-30B-A3B Load Balancing"
      ],
      "metadata": {
        "id": "H80J72j5tVWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "BCYUE_S9tcrR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_3nSeWytQF3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "import types\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from datasets import load_dataset\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "39mdpBn1ttrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.bfloat16\n",
        ").cuda()"
      ],
      "metadata": {
        "id": "SPAYbrRkuGOf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Inference Helpers"
      ],
      "metadata": {
        "id": "TCDtMrEFuMaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mmlu_subset(subject=\"all\", sample_size=50, seed=42):\n",
        "    \"\"\"\n",
        "    load a repeatable random subset of MMLU\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "    # load the entire dataset\n",
        "    dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
        "\n",
        "    # shuffle and select subset\n",
        "    indices = list(range(len(dataset)))\n",
        "    random.shuffle(indices)\n",
        "    subset_indices = indices[:sample_size]\n",
        "\n",
        "    return dataset.select(subset_indices)\n",
        "\n",
        "def format_mmlu_prompt(example):\n",
        "    \"\"\"\n",
        "    format question prompt with a primer at the end\n",
        "    \"\"\"\n",
        "    prompt = f\"Question: {example['question']}\\n\"\n",
        "    for i, choice in enumerate([\"A\", \"B\", \"C\", \"D\"]):\n",
        "        prompt += f\"{choice}. {example['choices'][i]}\\n\"\n",
        "    # prime the LLM to begin answering with the letter corresponding to its choice\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def evaluate_batch(examples, model, tokenizer):\n",
        "    \"\"\"\n",
        "    evaluate a local model on a batch of prompts\n",
        "    \"\"\"\n",
        "    # set pad token if doesn't exist\n",
        "    # need to pad because sequence lengths must be identical in batch\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # pad on left to ensure the last token is aligned across the batch\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    # format all prompts in batch\n",
        "    prompts = [format_mmlu_prompt(ex) for ex in examples]\n",
        "\n",
        "    # prepare inputs with padding\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "\n",
        "    # store mask on the model for access via MoE blocks\n",
        "    model.current_padding_mask = (inputs[\"attention_mask\"] == 0)\n",
        "\n",
        "    choice_tokens = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    choice_ids = [tokenizer.encode(token, add_special_tokens=False)[-1] for token in choice_tokens]\n",
        "\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    # measure event time\n",
        "    start_event.record()\n",
        "    with torch.inference_mode(): # inference_mode is faster than no_grad\n",
        "        # unpack inputs kwargs (\"input_ids\" and \"attention_mask\")\n",
        "        outputs = model(**inputs)\n",
        "        # grab logits for last token position for every sequence in batch\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "    end_event.record()\n",
        "\n",
        "    # must synchronize because GPU is async\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # extract logits for choice tokens across entire batch\n",
        "    choice_logits = logits[:, choice_ids] # keep first dimension (batch), select choice_ids from second dim\n",
        "\n",
        "    # get argmax for each row (dim=1)\n",
        "    prediction_indices = torch.argmax(choice_logits, dim=1).tolist() # calling .tolist() implicitly syncs, but it's good practice to explicitly sync\n",
        "\n",
        "    # map predictions to labels\n",
        "    predictions = [choice_tokens[idx] for idx in prediction_indices]\n",
        "\n",
        "    # measure total batch time\n",
        "    total_time = start_event.elapsed_time(end_event)\n",
        "\n",
        "    return predictions, total_time\n",
        "\n",
        "def evaluate_online(example, model, tokenizer):\n",
        "    \"\"\"\n",
        "    evaluate a local model on a single prompt\n",
        "    \"\"\"\n",
        "    prompt = format_mmlu_prompt(example)\n",
        "\n",
        "    # prepare inputs and choice tokens\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    choice_tokens = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    choice_ids = [tokenizer.encode(token, add_special_tokens=False)[-1] for token in choice_tokens]\n",
        "\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    # measure event time\n",
        "    start_event.record()\n",
        "    with torch.inference_mode(): # inference_mode is faster than no_grad\n",
        "        logits = model(**inputs).logits[:, -1, :]\n",
        "    end_event.record()\n",
        "\n",
        "    # must synchronize because GPU is async\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # extract logits for choice tokens\n",
        "    choice_logits = logits[0, choice_ids] # keep first dimension (batch), select choice_ids from second dim\n",
        "    prediction_idx = torch.argmax(choice_logits).item() # calling .item() implicitly syncs, but it's good practice to explicitly sync\n",
        "\n",
        "    # return tuple of selected answer and TTFT\n",
        "    return choice_tokens[prediction_idx], start_event.elapsed_time(end_event)\n",
        "\n",
        "def get_length_grouped_batches(dataset, tokenizer, batch_size=10):\n",
        "    \"\"\"\n",
        "    return groups of indices of prompts of size batch_size sorted in increasing order of prompt token count\n",
        "    \"\"\"\n",
        "    prompts = [format_mmlu_prompt(ex) for ex in dataset]\n",
        "    lengths = [len(tokenizer.encode(p, add_special_tokens=False)) for p in prompts] # use quick encoding\n",
        "\n",
        "    # sort indices by token count\n",
        "    sorted_indices = [i for i, _ in sorted(enumerate(lengths), key=lambda x: x[1])]\n",
        "\n",
        "    # create batches\n",
        "    batches = [sorted_indices[i:i + batch_size] for i in range(0, len(sorted_indices), batch_size)]\n",
        "\n",
        "    return batches\n",
        "\n",
        "def warmup(model, tokenizer):\n",
        "    \"\"\"\n",
        "    warm up the model by generating several small outputs\n",
        "    \"\"\"\n",
        "    prompt = \"Explain the theory of relativity in one sentence.\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    for _ in range(3):\n",
        "        _ = model.generate(**inputs, max_new_tokens=1)\n",
        "\n",
        "def remove_all_hooks(model):\n",
        "    \"\"\"\n",
        "    explicitly remove all hooks in the model\n",
        "    \"\"\"\n",
        "    for name, module in model.named_modules():\n",
        "        # clear forward hooks\n",
        "        module._forward_hooks.clear()\n",
        "        module._forward_pre_hooks.clear()\n",
        "\n",
        "        # clear backward hooks\n",
        "        module._backward_hooks.clear()\n",
        "        module._backward_pre_hooks.clear()\n",
        "\n",
        "def list_hooks(model, verbose=False):\n",
        "    \"\"\"\n",
        "    list all existing hooks in the model\n",
        "    \"\"\"\n",
        "    cnt = 0\n",
        "    for name, module in model.named_modules():\n",
        "        if module._forward_hooks or module._forward_pre_hooks or module._backward_hooks:\n",
        "            if verbose:\n",
        "                print(f\"Module: {name}\")\n",
        "                for hook_id, hook in module._forward_hooks.items():\n",
        "                    print(f\"  - Forward Hook (ID: {hook_id}): {hook}\")\n",
        "                for hook_id, hook in module._forward_pre_hooks.items():\n",
        "                    print(f\"  - Forward Pre-Hook (ID: {hook_id}): {hook}\")\n",
        "                for hook_id, hook in module._backward_hooks.items():\n",
        "                    print(f\"  - Backward Hook (ID: {hook_id}): {hook}\")\n",
        "            cnt += 1\n",
        "    if cnt > 0:\n",
        "        print(f\"Model has {cnt} hooks\")\n",
        "    else:\n",
        "        print(\"Model has no hooks\")"
      ],
      "metadata": {
        "id": "ltMqnafeuNU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Forward Function + Helpers"
      ],
      "metadata": {
        "id": "PNkvT3EAveLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimized_forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    drop-in replacement forward function for Qwen3MoeSparseMoeBlock\n",
        "    efficiently balances padding token distribution across experts\n",
        "    \"\"\"\n",
        "    batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
        "    # flatten 3D hidden_states tensor into 2D (batch_size * sequence_length, hidden_dim)\n",
        "    hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n",
        "    # grab batch_size * sequence_length as num_total_tokens\n",
        "    num_total_tokens = hidden_states_reshaped.size(0)\n",
        "\n",
        "    # get padding mask (opposite of attention mask) from parent_model (set during swap_forward)\n",
        "    mask = getattr(self.parent_model, \"current_padding_mask\", None)\n",
        "    if mask is not None:\n",
        "        is_padding = mask.view(-1)\n",
        "    else:\n",
        "        # fallback: assume all tokens are real (not padding)\n",
        "        is_padding = torch.zeros(num_total_tokens, device=hidden_states.device, dtype=torch.bool)\n",
        "\n",
        "    # call gate function and store routing weights + selected experts\n",
        "    _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n",
        "\n",
        "    # safely access number of experts and top k values\n",
        "    num_experts = self.num_experts if hasattr(self, 'num_experts') else self.parent_model.num_experts\n",
        "    top_k = getattr(self, 'top_k', getattr(self, 'num_experts_per_tok', 8)) # fallback: assume top k = 8\n",
        "\n",
        "    # grab indices of padding tokens (access 0th element because torch.where returns a tuple)\n",
        "    padding_indices = torch.where(is_padding != 0)[0]\n",
        "    # count number of elements in padding_indices\n",
        "    num_padding = padding_indices.numel()\n",
        "\n",
        "    if num_padding > 0:\n",
        "        real_mask = ~is_padding\n",
        "        # filter out padding and get expert assignments\n",
        "        real_expert_assignments = selected_experts[real_mask].view(-1)\n",
        "        # calculate expert load from real tokens\n",
        "        current_load = torch.bincount(real_expert_assignments, minlength=num_experts).float()\n",
        "\n",
        "        # total number of padding tokens we need to redistribute\n",
        "        total_slots_to_fill = num_padding * top_k\n",
        "\n",
        "        # calculate how far from average each expert's capacity is\n",
        "        avg_target = (current_load.sum() + total_slots_to_fill) / num_experts\n",
        "        deficits = (avg_target - current_load).clamp(min=0) # clamp to avoid negative capacity\n",
        "\n",
        "        # in case model is already perfectly balanced, fallback to uniform distribution\n",
        "        if deficits.sum() == 0:\n",
        "            deficits = torch.ones_like(deficits)\n",
        "\n",
        "        # allocate tokens proportionally based on the needs of each expert\n",
        "        scale_factor = total_slots_to_fill / deficits.sum()\n",
        "        fill_counts = (deficits * scale_factor).floor() # floor to ensure integer capacity\n",
        "\n",
        "        # fix rounding errors from floor\n",
        "        remainder = int(total_slots_to_fill - fill_counts.sum())\n",
        "\n",
        "        # distribute remainder to experts with highest original deficits\n",
        "        if remainder > 0:\n",
        "            _, top_indices = torch.topk(deficits, k=remainder)\n",
        "            fill_counts[top_indices] += 1\n",
        "\n",
        "        # create list of expert IDs based on fill_counts\n",
        "        new_padding_expert_ids = torch.repeat_interleave(\n",
        "            torch.arange(num_experts, device=hidden_states.device),\n",
        "            fill_counts.long()\n",
        "        )\n",
        "\n",
        "        # overwrite original padding token routing with new load balanced routing\n",
        "        selected_experts[padding_indices] = new_padding_expert_ids.view(num_padding, top_k)\n",
        "        # set equal importance to new padding assignments\n",
        "        routing_weights[padding_indices] = 1.0 / top_k\n",
        "\n",
        "    # set selected experts attribute\n",
        "    self.current_selected_experts = selected_experts\n",
        "\n",
        "    # compute outputs using load balanced token routing\n",
        "    final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n",
        "\n",
        "    return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
        "\n",
        "def swap_forward(model, new_forward_fn):\n",
        "    \"\"\"\n",
        "    swaps forward function in every MoE block\n",
        "    \"\"\"\n",
        "    swapped_count = 0\n",
        "\n",
        "    for i, layer in enumerate(model.model.layers):\n",
        "        moe_block = layer.mlp\n",
        "        # print(f\"Patching module at {id(moe_block)}\")\n",
        "\n",
        "        if hasattr(moe_block, 'gate'):\n",
        "            # attach reference to parent model\n",
        "            moe_block.parent_model = model\n",
        "\n",
        "            # store original forward on the block itself if not already done\n",
        "            if not hasattr(moe_block, 'original_forward'):\n",
        "                moe_block.original_forward = moe_block.forward\n",
        "\n",
        "            # bind new function to this block instance\n",
        "            moe_block.forward = types.MethodType(new_forward_fn, moe_block)\n",
        "            swapped_count += 1\n",
        "\n",
        "    print(f\"Swapped forward for {swapped_count} MoE blocks\")\n",
        "\n",
        "def restore_forward(model):\n",
        "    \"\"\"\n",
        "    restores original forward method for every MoE block\n",
        "    \"\"\"\n",
        "    restored_count = 0\n",
        "    for i, layer in enumerate(model.model.layers):\n",
        "        moe_block = layer.mlp\n",
        "        if hasattr(moe_block, 'original_forward'):\n",
        "            # restore original forward function\n",
        "            moe_block.forward = moe_block.original_forward\n",
        "\n",
        "            # delete original forward attribute\n",
        "            del moe_block.original_forward\n",
        "            restored_count += 1\n",
        "\n",
        "    print(f\"Restored original forward for {restored_count} MoE blocks\")"
      ],
      "metadata": {
        "id": "wnsnIjcbvgGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Data Structures"
      ],
      "metadata": {
        "id": "XMFgoQjAugBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BatchRun:\n",
        "    \"\"\"\n",
        "    stores info about a single eval run\n",
        "    \"\"\"\n",
        "    evals: List[str] # names of the evals (e.g. unsorted_default, sorted_default, unsorted_optimized, sorted_optimized)\n",
        "    sorted_flags: List[bool] # whether or not data is sorted for a certain eval\n",
        "    opt_flags: List[bool] # whether or not to use optimized forward function\n",
        "    batch_size: int = 10\n",
        "    num_batches: int = 10\n",
        "\n",
        "    def __post_init__(self):\n",
        "        length = len(self.evals)\n",
        "        assert all([len(l) == length for l in [self.sorted_flags, self.opt_flags]]), \"Lengths of input lists do not match\"\n",
        "\n",
        "class Evaluation:\n",
        "    \"\"\"\n",
        "    runs evals and generates visuals over several BatchRuns\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer, runs: List[BatchRun]):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.runs = runs\n",
        "        self.monitors = [{name: MoEMonitor(self.model, optimized=opt) for name, opt in zip(run.evals, run.opt_flags)} for run in runs]\n",
        "        self.accuracies = [defaultdict(list) for _ in range(len(runs))]\n",
        "        self.cvs = [{} for _ in range(len(runs))]\n",
        "        self.batch_times = [defaultdict(list) for _ in range(len(runs))]\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        perform evaluations based on BatchRuns specified in self.runs\n",
        "        \"\"\"\n",
        "        # warm up model\n",
        "        warmup(model, tokenizer)\n",
        "\n",
        "        # remove all existing hooks so there are no conflicts\n",
        "        remove_all_hooks(self.model)\n",
        "        list_hooks(self.model)\n",
        "\n",
        "        for i, run in enumerate(self.runs):\n",
        "            print(f\"\\n============Begin run {i+1} (Batch={run.batch_size}x{run.num_batches})============\")\n",
        "\n",
        "            # load batches from dataset\n",
        "            batch_size = run.batch_size\n",
        "            num_batches = run.num_batches\n",
        "            full_subset = load_mmlu_subset(sample_size=batch_size * num_batches)\n",
        "\n",
        "            # create sorted and unsorted batches\n",
        "            batch_indices = get_length_grouped_batches(full_subset, tokenizer, batch_size=batch_size)\n",
        "            sorted_batches = [[full_subset[i] for i in indices] for indices in batch_indices]\n",
        "            unsorted_batches = [[full_subset[i * batch_size + j] for j in range(batch_size)] for i in range(num_batches)]\n",
        "\n",
        "            for j, eval in enumerate(run.evals):\n",
        "                print(f\"\\n------------Begin eval {eval}------------\")\n",
        "                self.monitors[i][eval].attach()\n",
        "\n",
        "                # check if need to swap forward function\n",
        "                if run.opt_flags[j]:\n",
        "                    swap_forward(self.model, optimized_forward)\n",
        "\n",
        "                # use sorted or unsorted based on flags\n",
        "                batches = sorted_batches if run.sorted_flags[j] else unsorted_batches\n",
        "\n",
        "                # run batch inference\n",
        "                for batch in batches:\n",
        "                    choices, total_batch_time = evaluate_batch(batch, model, tokenizer)\n",
        "                    # store batch times\n",
        "                    self.batch_times[i][eval].append(total_batch_time)\n",
        "\n",
        "                    # store accuracies\n",
        "                    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
        "                    ground_truths = [labels[ex[\"answer\"]] for ex in batch] # can precompute these\n",
        "                    self.accuracies[i][eval].append(np.equal(ground_truths, choices).sum() / batch_size)\n",
        "\n",
        "                # store CVs\n",
        "                self.cvs[i][eval] = self.monitors[i][eval].stats.get_all_layer_cvs()\n",
        "\n",
        "                # restore forward function\n",
        "                if run.opt_flags[j]:\n",
        "                    restore_forward(self.model)\n",
        "\n",
        "                self.monitors[i][eval].remove()\n",
        "\n",
        "    def _base_plot(self, data_source, title_suffix, ylabel, x_label_type=\"layer\", colors=[\"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\"], cols=2):\n",
        "        \"\"\"\n",
        "        generic plotting function\n",
        "        data_source: List[Dict] where data_source[run_idx][eval_name] contains the y-values\n",
        "        x_label_type: \"layer\" (for MoE layer indices) or \"batch\" (for iteration indices)\n",
        "        \"\"\"\n",
        "        num_runs = len(self.runs)\n",
        "        rows = math.ceil(num_runs / cols)\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 4 * rows), squeeze=False)\n",
        "\n",
        "        # standardize axes into a flat list\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # determine global y limit\n",
        "        all_values = [v for run_data in data_source for v in run_data.values()]\n",
        "        flat_values = [item for sublist in all_values for item in sublist]\n",
        "        max_yval = max(flat_values) if flat_values else 1.0\n",
        "\n",
        "        for i, run in enumerate(self.runs):\n",
        "            ax = axes[i]\n",
        "\n",
        "            for j, eval_name in enumerate(run.evals):\n",
        "                y_values = data_source[i][eval_name]\n",
        "                print(f\"Average {ylabel} for {eval_name} (Batch {run.batch_size}x{run.num_batches}): {sum(y_values) / len(y_values):.2f}\")\n",
        "                x_values = range(len(y_values))\n",
        "\n",
        "                ax.plot(\n",
        "                    x_values, y_values,\n",
        "                    label=eval_name,\n",
        "                    color=colors[j % len(colors)],\n",
        "                    marker=\"o\" if x_label_type == \"layer\" else \"x\",\n",
        "                    linewidth=2 if x_label_type == \"layer\" else 1.5,\n",
        "                    markersize=4,\n",
        "                    alpha=0.8\n",
        "                )\n",
        "\n",
        "            # subplot formatting\n",
        "            ax.set_title(f\"Run {i+1}: Batch {run.batch_size}x{run.num_batches} {title_suffix}\", fontsize=12)\n",
        "            ax.set_ylabel(ylabel, fontsize=10)\n",
        "            ax.set_ylim(0, max_yval * 1.1)\n",
        "            ax.legend(frameon=True, fontsize=8)\n",
        "            ax.grid(True, linestyle=\"--\" if x_label_type == \"layer\" else \":\", alpha=0.4)\n",
        "\n",
        "            # x-axis logic\n",
        "            ax.set_xticks(range(0, len(y_values), max(1, len(y_values) // 10)))\n",
        "            if x_label_type == \"layer\":\n",
        "                ax.set_xlabel(\"Layer Index\", fontsize=10)\n",
        "            else:\n",
        "                ax.set_xlabel(\"Batch Index\", fontsize=10)\n",
        "\n",
        "        # clean up empty subplots\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            axes[j].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig, axes\n",
        "\n",
        "    def plot_cv(self, cols=2):\n",
        "        \"\"\"\n",
        "        plot expert load imbalance (CV) per layer\n",
        "        \"\"\"\n",
        "        return self._base_plot(\n",
        "            data_source=self.cvs,\n",
        "            title_suffix=\"(CV)\",\n",
        "            ylabel=\"CV\",\n",
        "            x_label_type=\"layer\",\n",
        "            cols=cols\n",
        "        )\n",
        "\n",
        "    def plot_batch_times(self, cols=2):\n",
        "        \"\"\"\n",
        "        plot execution time per batch\n",
        "        \"\"\"\n",
        "        return self._base_plot(\n",
        "            data_source=self.batch_times,\n",
        "            title_suffix=\"(Time)\",\n",
        "            ylabel=\"Execution Time (ms)\",\n",
        "            x_label_type=\"batch\",\n",
        "            # colors=[\"#2d6a4f\", \"#95d5b2\", \"#6a0dad\", \"#b39ddb\"],\n",
        "            cols=cols\n",
        "        )\n",
        "\n",
        "    def plot_accuracies(self, cols=2):\n",
        "        \"\"\"\n",
        "        plot accuracy per batch\n",
        "        \"\"\"\n",
        "        return self._base_plot(\n",
        "            data_source=self.accuracies,\n",
        "            title_suffix=\"(Accuracy)\",\n",
        "            ylabel=\"Accuracy (Proportion)\",\n",
        "            x_label_type=\"batch\",\n",
        "            # colors=[\"#b91d1d\", \"#f87171\", \"#4b5563\", \"#9ca3af\"],\n",
        "            cols=cols\n",
        "        )\n",
        "\n",
        "class MoELoadStats:\n",
        "    def __init__(self, distribution):\n",
        "        assert distribution.ndim == 2, \"Distribution must be two-dimensional\"\n",
        "        self.distribution = distribution\n",
        "\n",
        "    def plot_heatmap(self, title=\"MoE Token Distribution\", save_path=\"distribution_heatmap.png\", cmap=\"magma\"):\n",
        "        \"\"\"\n",
        "        plot heatmap of per-layer per-expert token distribution\n",
        "        \"\"\"\n",
        "        # move pytorch tensor to CPU and convert to numpy array\n",
        "        data_to_plot = self.distribution.detach().cpu().numpy()\n",
        "        layers, experts = data_to_plot.shape\n",
        "\n",
        "        # set width based on expert count\n",
        "        fig_width = max(12, experts * 0.2)\n",
        "        plt.figure(figsize=(fig_width, 8))\n",
        "\n",
        "        # use nearest interpolation\n",
        "        im = plt.imshow(data_to_plot, aspect='auto', cmap=cmap, interpolation='nearest')\n",
        "\n",
        "        plt.title(title, fontsize=16, pad=20)\n",
        "        plt.xlabel('Expert Index', fontsize=24)\n",
        "        plt.ylabel('Layer Index', fontsize=24)\n",
        "\n",
        "        # use steps if more than 32 experts\n",
        "        if experts > 32:\n",
        "            step = 8 if experts <= 128 else 16\n",
        "            tick_indices = np.arange(0, experts, step)\n",
        "            plt.xticks(tick_indices, tick_indices, fontsize=10)\n",
        "        else:\n",
        "            plt.xticks(range(experts))\n",
        "\n",
        "        plt.yticks(range(layers))\n",
        "\n",
        "        # add grid to distinguish expert boundaries\n",
        "        plt.gca().set_xticks(np.arange(-.5, experts, 1), minor=True)\n",
        "        plt.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=0.5, alpha=0.2)\n",
        "        plt.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "        cbar = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "        cbar.set_label('Token Count', rotation=270, labelpad=15)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300)\n",
        "\n",
        "    def get_layer_imbalance(self, layer_idx: int):\n",
        "        \"\"\"\n",
        "        compute CV (coefficient of variance) for a specific layer\n",
        "        \"\"\"\n",
        "        num_layers = self.distribution.shape[0]\n",
        "\n",
        "        # ensure valid layer index\n",
        "        assert 0 <= layer_idx < num_layers, \"Invalid layer index\"\n",
        "\n",
        "        layer_data = self.distribution[layer_idx].float()\n",
        "        return layer_data.std() / (layer_data.mean() + 1e-6)\n",
        "\n",
        "    def get_all_layer_cvs(self):\n",
        "        \"\"\"\n",
        "        get CV for every layer in the distribution\n",
        "        \"\"\"\n",
        "        num_layers = self.distribution.shape[0]\n",
        "        return [self.get_layer_imbalance(i).item() for i in range(num_layers)]\n",
        "\n",
        "class MoEMonitor:\n",
        "    def __init__(self, model, optimized=False):\n",
        "        self.model = model\n",
        "        self.handles = []\n",
        "        self.optimized = optimized\n",
        "\n",
        "        self.num_layers = model.config.num_hidden_layers\n",
        "        self.num_experts = model.config.num_experts\n",
        "\n",
        "        # initialize stats with tensor of all zeros\n",
        "        self.stats = MoELoadStats(\n",
        "            distribution=torch.zeros(\n",
        "                (self.num_layers, self.num_experts),\n",
        "                device=model.device,\n",
        "                dtype=torch.long\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _gate_hook_fn(self, layer_idx):\n",
        "        \"\"\"\n",
        "        return hook function that is meant for hooking into gate module\n",
        "        \"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # get router logits, data structure may vary based on architecture\n",
        "            router_logits = output[0] if isinstance(output, tuple) else output\n",
        "\n",
        "            # identify which experts were selected\n",
        "            _, selected_experts = torch.topk(router_logits, k=module.top_k, dim=-1)\n",
        "\n",
        "            # flatten indices and create ones tensor (gate)\n",
        "            indices = selected_experts.view(-1)\n",
        "            ones = torch.ones_like(indices, dtype=torch.long)\n",
        "\n",
        "            # perform in-place scatter add\n",
        "            self.stats.distribution[layer_idx].scatter_add_(0, indices, ones)\n",
        "        return hook\n",
        "\n",
        "    def _mlp_hook_fn(self, layer_idx):\n",
        "        \"\"\"\n",
        "        return hook function that is meant for hooking into mlp module\n",
        "        \"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # look for selected experts attribute\n",
        "            selected_experts = getattr(module, \"current_selected_experts\", None)\n",
        "            # print(f\"Hooking module at {id(module)}\")\n",
        "            if selected_experts is None:\n",
        "                print(f\"Layer {layer_idx}: 'current_selected_experts' attribute not found on {type(module)}\")\n",
        "                return\n",
        "\n",
        "            # flatten indices and create ones tensor (mlp)\n",
        "            indices = selected_experts.view(-1)\n",
        "            ones = torch.ones_like(indices, dtype=torch.long)\n",
        "\n",
        "            # perform in-place scatter add\n",
        "            self.stats.distribution[layer_idx].scatter_add_(0, indices, ones)\n",
        "        return hook\n",
        "\n",
        "    def attach(self):\n",
        "        \"\"\"\n",
        "        attaches hooks to the mlp/gate of every MoE layer\n",
        "        \"\"\"\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            if self.optimized: # need to hook onto mlp layer because that's where token rerouting happens\n",
        "                mlp_module = self.model.model.layers[layer_idx].mlp\n",
        "                handle = mlp_module.register_forward_hook(self._mlp_hook_fn(layer_idx))\n",
        "            else: # otherwise just grab expert selection from gate\n",
        "                gate_module = self.model.model.layers[layer_idx].mlp.gate\n",
        "                handle = gate_module.register_forward_hook(self._gate_hook_fn(layer_idx))\n",
        "            self.handles.append(handle)\n",
        "        print(f\"Monitoring {self.num_layers} MoE layers ({'mlp' if self.optimized else 'gate'})\")\n",
        "\n",
        "    def remove(self):\n",
        "        \"\"\"\n",
        "        safely detaches all hooks\n",
        "        \"\"\"\n",
        "        for handle in self.handles:\n",
        "            handle.remove()\n",
        "        self.handles = []\n",
        "        print(\"Removed all handles\")"
      ],
      "metadata": {
        "id": "QgNF99QluhqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Evaluations"
      ],
      "metadata": {
        "id": "ULsewnNM78Yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Evaluation (Baseline)"
      ],
      "metadata": {
        "id": "hKXaI4jjshzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure model forward is restored to original\n",
        "restore_forward(model)\n",
        "\n",
        "evals = [\"Unsorted\", \"Sorted\"]\n",
        "is_sorted = [False, True]\n",
        "is_opt = [False, False]\n",
        "\n",
        "runs = [\n",
        "    # batch size powers of 2, num batches decreasing\n",
        "    BatchRun(evals, is_sorted, is_opt, batch_size=8, num_batches=50),\n",
        "    BatchRun(evals, is_sorted, is_opt, batch_size=16, num_batches=20),\n",
        "    BatchRun(evals, is_sorted, is_opt, batch_size=32, num_batches=10),\n",
        "    BatchRun(evals, is_sorted, is_opt, batch_size=64, num_batches=5),\n",
        "]\n",
        "\n",
        "initial_eval = Evaluation(model, tokenizer, runs)\n",
        "initial_eval.evaluate()\n",
        "\n",
        "initial_cv_plot_fig, initial_cv_plot_ax = initial_eval.plot_cv(cols=4)\n",
        "initial_timing_plot_fig, initial_timing_plot_ax = initial_eval.plot_batch_times(cols=4)\n",
        "initial_acc_plot_fig, initial_acc_plot_ax = initial_eval.plot_accuracies(cols=4)"
      ],
      "metadata": {
        "id": "dc5KCCzBse0f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Evaluation (Optimized vs Baseline)"
      ],
      "metadata": {
        "id": "Fe7cG9pFskFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure model forward is restored to original\n",
        "restore_forward(model)\n",
        "\n",
        "evals = [\"Unsorted (Unoptimized)\", \"Sorted (Unoptimized)\", \"Unsorted (Optimized)\", \"Sorted (Optimized)\"]\n",
        "is_sorted = [False, True, False, True]\n",
        "is_opt = [False, False, True, True]\n",
        "\n",
        "runs = [\n",
        "    # batch size powers of 2, num batches decreasing\n",
        "    BatchRun(evals, is_sorted, is_opt, batch_size=8, num_batches=50),\n",
        "    BatchRun(evals, is_sorted, is_opt, batch_size=16, num_batches=20),\n",
        "    BatchRun(evals, is_sorted, is_opt, batch_size=32, num_batches=10),\n",
        "    BatchRun(evals, is_sorted, is_opt, batch_size=64, num_batches=5),\n",
        "]\n",
        "\n",
        "final_eval = Evaluation(model, tokenizer, runs)\n",
        "final_eval.evaluate()\n",
        "\n",
        "final_cv_plot_fig, final_cv_plot_ax = final_eval.plot_cv(cols=4)\n",
        "final_timing_plot_fig, final_timing_plot_ax = final_eval.plot_batch_times(cols=4)\n",
        "final_acc_plot_fig, final_acc_plot_ax = final_eval.plot_accuracies(cols=4)"
      ],
      "metadata": {
        "id": "PAN92uKQ7_Uq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Evaluation Plots"
      ],
      "metadata": {
        "id": "-65FP0kMBiqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_cv_plot_fig.savefig('initial_cv_plots.png', dpi=300, bbox_inches='tight')\n",
        "initial_timing_plot_fig.savefig('initial_timing_plots.png', dpi=300, bbox_inches='tight')\n",
        "initial_acc_plot_fig.savefig('initial_accuracy_plots.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "final_cv_plot_fig.savefig('final_cv_plots.png', dpi=300, bbox_inches='tight')\n",
        "final_timing_plot_fig.savefig('final_timing_plots.png', dpi=300, bbox_inches='tight')\n",
        "final_acc_plot_fig.savefig('final_accuracy_plots.png', dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "HM96JigaEQDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Visuals"
      ],
      "metadata": {
        "id": "v1-VAi1wzOi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Padding Tokens in Batch\n",
        "- note that for one batch, sorting does not change padding token count"
      ],
      "metadata": {
        "id": "yfyfwaNDRGZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_padding(attention_mask: torch.Tensor, batch_idx: int, sorted: bool, pad_perc: int, save_path: str = \"padding_map.png\"):\n",
        "    \"\"\"\n",
        "    creates a grid visualization of real vs padding tokens in a batch\n",
        "    attention_mask: Tensor of shape (batch_size, seq_len)\n",
        "    \"\"\"\n",
        "    # convert tensor to numpy\n",
        "    mask_data = attention_mask.detach().cpu().numpy()\n",
        "    batch_size, seq_len = mask_data.shape\n",
        "\n",
        "    # use orange for padding and blue for real tokens\n",
        "    cmap = ListedColormap(['#ffa500', '#3498db'])\n",
        "\n",
        "    # set plot size\n",
        "    plt.figure(figsize=(10,4))\n",
        "\n",
        "    # create plot\n",
        "    plt.imshow(mask_data, aspect='auto', cmap=cmap, interpolation='nearest')\n",
        "\n",
        "    # add grid lines to make token boundaries distinct\n",
        "    plt.gca().set_xticks(np.arange(-.5, seq_len, 1), minor=True)\n",
        "    plt.gca().set_yticks(np.arange(-.5, batch_size, 1), minor=True)\n",
        "    plt.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=1, alpha=0.5)\n",
        "    plt.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "    # labeling\n",
        "    plt.title(f'Batch {batch_idx} ({\"Sorted\" if sorted else \"Unsorted\"}): {pad_perc:.2f}% Padding', fontsize=10, pad=15)\n",
        "    plt.xlabel('Token Index', fontsize=8)\n",
        "    plt.ylabel('Prompt Index', fontsize=8)\n",
        "\n",
        "    # create custom legend\n",
        "    # from matplotlib.patches import Patch\n",
        "    # legend_elements = [\n",
        "    #     Patch(facecolor='#3498db', label='Real Token'),\n",
        "    #     Patch(facecolor='#ffa500', label='Padding Token')\n",
        "    # ]\n",
        "    # plt.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.2, 1))\n",
        "\n",
        "    # save output\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# load batches from dataset\n",
        "batch_size = 10\n",
        "num_batches = 3\n",
        "full_subset = load_mmlu_subset(sample_size=batch_size * num_batches)\n",
        "\n",
        "# create sorted and unsorted batches\n",
        "batch_indices = get_length_grouped_batches(full_subset, tokenizer, batch_size=batch_size)\n",
        "sorted_batches = [[full_subset[i] for i in indices] for indices in batch_indices]\n",
        "unsorted_batches = [[full_subset[i * batch_size + j] for j in range(batch_size)] for i in range(num_batches)]\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# visualize unsorted batches\n",
        "for i, batch in enumerate(unsorted_batches):\n",
        "    prompts = [format_mmlu_prompt(ex) for ex in batch]\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    save_name = f\"unsorted_batch_{i+1}_of_{num_batches}.png\"\n",
        "    print(save_name)\n",
        "\n",
        "    num_pad_tok = (inputs[\"attention_mask\"] == 0).sum()\n",
        "    print(f\"Num padding tokens: {num_pad_tok}\")\n",
        "    pad_perc = num_pad_tok / torch.ones_like(inputs[\"attention_mask\"]).sum() * 100\n",
        "    print(f\"{pad_perc=}\")\n",
        "\n",
        "    visualize_padding(\n",
        "        inputs[\"attention_mask\"],\n",
        "        batch_idx=i+1,\n",
        "        sorted=False,\n",
        "        pad_perc=pad_perc,\n",
        "        save_path=save_name\n",
        "    )\n",
        "\n",
        "# visualize sorted batches\n",
        "for i, batch in enumerate(sorted_batches):\n",
        "    prompts = [format_mmlu_prompt(ex) for ex in batch]\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    save_name = f\"sorted_batch_{i+1}_of_{num_batches}.png\"\n",
        "    print(save_name)\n",
        "\n",
        "    num_pad_tok = (inputs[\"attention_mask\"] == 0).sum()\n",
        "    print(f\"Num padding tokens: {num_pad_tok}\")\n",
        "    pad_perc = num_pad_tok / torch.ones_like(inputs[\"attention_mask\"]).sum() * 100\n",
        "    print(f\"{pad_perc=}\")\n",
        "\n",
        "    visualize_padding(\n",
        "        inputs[\"attention_mask\"],\n",
        "        batch_idx=i+1,\n",
        "        sorted=True,\n",
        "        pad_perc=pad_perc,\n",
        "        save_path=save_name\n",
        "    )\n"
      ],
      "metadata": {
        "id": "s5GguXr8RKPo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Pad Token Redistribution"
      ],
      "metadata": {
        "id": "Jg6p7zGeCZRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 experts, top-k = 2\n",
        "experts = np.arange(6)\n",
        "expert_labels = [f'Expert {i}' for i in experts]\n",
        "\n",
        "# initial \"real\" token counts\n",
        "real_tokens = np.array([100, 120, 90, 110, 130, 100])\n",
        "\n",
        "# initial padding tokens\n",
        "pad_tokens_initial = np.array([0, 200, 0, 0, 200, 0])\n",
        "\n",
        "# calculate totals\n",
        "total_real = np.sum(real_tokens)\n",
        "total_pad = np.sum(pad_tokens_initial)\n",
        "total_tokens_global = total_real + total_pad\n",
        "num_experts = len(experts)\n",
        "target_avg = total_tokens_global / num_experts\n",
        "\n",
        "pad_tokens_optimized = np.zeros_like(real_tokens)\n",
        "\n",
        "# get balanced pad token counts\n",
        "for i in range(num_experts):\n",
        "    needed = target_avg - real_tokens[i]\n",
        "    pad_tokens_optimized[i] = needed\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(6, 10), sharey=True)\n",
        "\n",
        "color_real = '#3498db'\n",
        "color_pad = '#ffa500'\n",
        "\n",
        "# plot initial load\n",
        "axes[0].bar(experts, real_tokens, color=color_real, label='Real Tokens')\n",
        "axes[0].bar(experts, pad_tokens_initial, bottom=real_tokens, color=color_pad, label='Pad Tokens')\n",
        "axes[0].set_title('Initial Expert Load (Imbalanced Padding)', fontsize=14)\n",
        "axes[0].set_ylabel('Token Count', fontsize=12)\n",
        "axes[0].set_xlabel('Expert Index', fontsize=12)\n",
        "axes[0].set_xticks(experts)\n",
        "axes[0].grid(axis='y', linestyle='--', alpha=0.5)\n",
        "# axes[0].legend()\n",
        "\n",
        "# plot optimized load\n",
        "axes[1].bar(experts, real_tokens, color=color_real, label='Real Tokens')\n",
        "axes[1].bar(experts, pad_tokens_optimized, bottom=real_tokens, color=color_pad, label='Pad Tokens')\n",
        "axes[1].set_title('Optimized Expert Load (Redistributed Padding)', fontsize=14)\n",
        "axes[1].set_ylabel('Token Count', fontsize=12)\n",
        "axes[1].set_xlabel('Expert Index', fontsize=12)\n",
        "axes[1].set_xticks(experts)\n",
        "axes[1].grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# plt.suptitle('MoE Batch Inference: Padding Token Redistribution Optimization', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q-cP69klzSI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [ARCHIVED] Plot Per-Layer Per-Expert Token Distributions"
      ],
      "metadata": {
        "id": "fkBsETQLuWUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Inference on Sample Text"
      ],
      "metadata": {
        "id": "F6TFXXKmz_XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warmup(model, tokenizer)\n",
        "\n",
        "# remove all existing hooks so there are no conflicts\n",
        "remove_all_hooks(model)\n",
        "list_hooks(model)\n",
        "\n",
        "# initialize monitors\n",
        "monitor = {\n",
        "    \"batch_unsorted\": MoEMonitor(model),\n",
        "    \"batch_sorted\": MoEMonitor(model),\n",
        "    \"online\": MoEMonitor(model)\n",
        "}\n",
        "\n",
        "# load repeatable subset from dataset\n",
        "subset_size = 32\n",
        "num_subsets = 10\n",
        "full_subset = load_mmlu_subset(sample_size=subset_size * num_subsets)\n",
        "\n",
        "# get length optimized indices\n",
        "batch_indices = get_length_grouped_batches(full_subset, tokenizer, batch_size=subset_size)\n",
        "sorted_subsets = [[full_subset[i] for i in indices] for indices in batch_indices]\n",
        "unsorted_subsets = [[full_subset[i * subset_size + j] for j in range(subset_size)] for i in range(num_subsets)]\n",
        "\n",
        "# initialize accuracy metrics\n",
        "correct_count = {\n",
        "    \"batch_unsorted\": 0,\n",
        "    \"batch_sorted\": 0,\n",
        "    \"online\": 0\n",
        "}\n",
        "\n",
        "# batch inference unsorted\n",
        "print(f\"\\nBegin UNSORTED batch inference ({subset_size=}, {num_subsets=})\")\n",
        "monitor[\"batch_unsorted\"].attach()\n",
        "\n",
        "total_batch_times = torch.zeros(num_subsets)\n",
        "for i, subset in enumerate(unsorted_subsets): # unsorted subsets\n",
        "    choices, total_batch_time = evaluate_batch(subset, model, tokenizer)\n",
        "    total_batch_times[i] = total_batch_time\n",
        "    correct_count[\"batch_unsorted\"] += sum([1 for i, example in enumerate(subset) if [\"A\", \"B\", \"C\", \"D\"][example['answer']] == choices[i]])\n",
        "print(f\"\\n(Batch) Final Accuracy: {correct_count[\"batch_unsorted\"]/(len(subset) * num_subsets):.2%}\")\n",
        "print(f\"(Batch) Average Total Batch Time: {total_batch_time:.2f} ms\") # ISSUE: not using total_batch_times\n",
        "\n",
        "monitor[\"batch_unsorted\"].remove()\n",
        "\n",
        "# batch inference sorted\n",
        "print(f\"\\nBegin SORTED batch inference ({subset_size=}, {num_subsets=})\")\n",
        "monitor[\"batch_sorted\"].attach()\n",
        "\n",
        "total_batch_times = torch.zeros(num_subsets)\n",
        "for i, subset in enumerate(sorted_subsets): # sorted subsets\n",
        "    choices, total_batch_time = evaluate_batch(subset, model, tokenizer)\n",
        "    total_batch_times[i] = total_batch_time\n",
        "    correct_count[\"batch_sorted\"] += sum([1 for i, example in enumerate(subset) if [\"A\", \"B\", \"C\", \"D\"][example['answer']] == choices[i]])\n",
        "print(f\"\\n(Batch) Final Accuracy: {correct_count[\"batch_sorted\"]/(len(subset) * num_subsets):.2%}\")\n",
        "print(f\"(Batch) Average Total Batch Time: {total_batch_time:.2f} ms\") # ISSUE: not using total_batch_times\n",
        "\n",
        "monitor[\"batch_sorted\"].remove()\n",
        "\n",
        "# online (non-batch) inference\n",
        "print(f\"\\nBegin online inference ({subset_size=}, {num_subsets=})\")\n",
        "monitor[\"online\"].attach()\n",
        "\n",
        "ttfts = torch.zeros(subset_size)\n",
        "for subset_idx, subset in enumerate(sorted_subsets): # sorted subsets\n",
        "    # print(f\"begin subset {subset_idx}\")\n",
        "    for i, example in enumerate(subset):\n",
        "        ground_truth = [\"A\", \"B\", \"C\", \"D\"][example['answer']]\n",
        "\n",
        "        pred, ttft = evaluate_online(example, model, tokenizer)\n",
        "        ttfts[i] = ttft\n",
        "\n",
        "        is_correct = (pred == ground_truth)\n",
        "        if is_correct: correct_count[\"online\"] += 1\n",
        "\n",
        "        # print(f\"[{i+1}] Pred: {pred} | Actual: {ground_truth} | {'' if is_correct else ''}\")\n",
        "print(f\"\\n(Online) Final Accuracy: {correct_count[\"online\"]/(len(subset) * num_subsets):.2%}\")\n",
        "print(f\"(Online) Average TTFT: {ttfts.mean():.2f} ms\")\n",
        "\n",
        "monitor[\"online\"].remove()"
      ],
      "metadata": {
        "id": "dWvdDSvtzr5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Coefficients of Variation (CVs)"
      ],
      "metadata": {
        "id": "XP52FepsPIfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = model.config.num_hidden_layers\n",
        "batch_unsorted_cvs = [monitor[\"batch_unsorted\"].stats.get_layer_imbalance(layer_idx=i).item() for i in range(num_layers)]\n",
        "batch_sorted_cvs = [monitor[\"batch_sorted\"].stats.get_layer_imbalance(layer_idx=i).item() for i in range(num_layers)]\n",
        "online_cvs = [monitor[\"online\"].stats.get_layer_imbalance(layer_idx=i).item() for i in range(num_layers)]\n",
        "\n",
        "plt.plot(range(num_layers), batch_unsorted_cvs, label='Batch Inference (unsorted)', color='#e74c3c', marker='o', linewidth=2)\n",
        "plt.plot(range(num_layers), batch_sorted_cvs, label='Batch Inference (sorted)', color='#ffa500', marker='o', linewidth=2)\n",
        "plt.plot(range(num_layers), online_cvs, label='Online Inference', color='#3498db', marker='s', linestyle='--', linewidth=2)\n",
        "\n",
        "plt.title('Expert Load Imbalance Across Layers ($CV$)', fontsize=14, pad=15)\n",
        "plt.xlabel('Layer Index', fontsize=12)\n",
        "plt.ylabel('Coefficient of Variation ($CV$)', fontsize=12)\n",
        "plt.legend(frameon=True)\n",
        "plt.grid(True, which='both', linestyle='--', alpha=0.4)\n",
        "\n",
        "plt.ylim(0, max(max(batch_unsorted_cvs), max(batch_sorted_cvs), max(online_cvs)) * 1.1)\n",
        "plt.xticks(range(0, num_layers, max(1, num_layers // 10)))\n",
        "\n",
        "plt.savefig(f'moe_cv_comparison_batch_{subset_size}_x{num_subsets}.png', dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "kGuRayeBPQbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Distribution\n",
        "- heatmap for unsorted batch inference is more polarized due to padding tokens being routed to the same set of 8 experts\n",
        "- heatmap for sorted batch inference is much closer (if not better than!?) ground truth\n",
        "- heatmap for online inference is used as ground truth because there are no padding tokens"
      ],
      "metadata": {
        "id": "mcxxeAOX7PEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monitor[\"batch_unsorted\"].stats.plot_heatmap(title=f\"Unsorted Batch\", save_path=f\"batch_inf_unsorted_{subset_size}x{num_subsets}\")\n",
        "monitor[\"batch_sorted\"].stats.plot_heatmap(title=f\"Sorted Batch\", save_path=f\"batch_inf_sorted_{subset_size}x{num_subsets}\")\n",
        "monitor[\"online\"].stats.plot_heatmap(title=f\"Sorted Online\", save_path=f\"online_inf_sorted_{subset_size}x{num_subsets}\")"
      ],
      "metadata": {
        "id": "df4RU_ct7OKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}